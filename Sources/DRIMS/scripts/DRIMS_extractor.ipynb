{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from flatten_json import flatten\n",
    "import re\n",
    "import geopandas as gpd\n",
    "import geopandas as gpd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r'D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output'\n",
    "\n",
    "# Function to extract the 'yyyy-mm' from the filename\n",
    "def extract_timeperiod(filename):\n",
    "    basename = os.path.basename(filename)\n",
    "    return basename[:7]  # Assumes the format 'yyyy-mm' at the start of the filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to flatten the JSON into a dataframe\n",
    "def extract_json(data, timeperiod, file_path):\n",
    "    try:\n",
    "        # Normalize specific fields from JSON and create a dataframe\n",
    "        data_file = json.load(data)\n",
    "\n",
    "        affected_pop = pd.DataFrame(data_file[\"affectedPopulation\"])\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        # Iterate over each entry in the 'details' Series\n",
    "        for index, row in affected_pop.iterrows():\n",
    "            district = row['district']  # Capture the district for the current row\n",
    "            details = row['details']  # Capture the details for the current row\n",
    "            \n",
    "            # Find matches for the current details string\n",
    "            match = re.findall(r'\\(\\s*(.*?)\\s*\\)', details)\n",
    "            \n",
    "            for m in match:\n",
    "                try:\n",
    "                    # Split the extracted string by ' | ' to get the revenue circle, population affected, and crop area\n",
    "                    revenue_circle, population_affected, crop_area = m.split(' | ')\n",
    "                    population_affected = int(re.search(r'\\d+', population_affected).group())\n",
    "                    crop_area = float(re.search(r'\\d+(\\.\\d+)?', crop_area).group())\n",
    "                    \n",
    "                    # Append the results as a dictionary to the list, including the correct district\n",
    "                    data_list.append({\n",
    "                        \"district\": district,  # Add the correct district value here\n",
    "                        \"revenue_circle\": revenue_circle.strip(),\n",
    "                        \"Population Affected\": population_affected,\n",
    "                        \"Crop Area\": crop_area\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(\"Error processing match:\", m, e)\n",
    "\n",
    "        # Create a DataFrame from the collected data\n",
    "        pop_crop = pd.DataFrame(data_list)\n",
    "\n",
    "        # human lives lost\n",
    "        hll = data_file[\"hllDetails\"]\n",
    "        df = pd.DataFrame(hll)\n",
    "\n",
    "        # Function to extract the details column into a structured DataFrame\n",
    "        def extract_details(df_column, lives_lost_col):\n",
    "            extracted_data = []\n",
    "            \n",
    "            for row in df_column:\n",
    "                district = row['district']\n",
    "                details = row['details']\n",
    "                # Extract the revenue circle and lives lost information\n",
    "                for match in details.split('), '):\n",
    "                    match = match.strip('()')\n",
    "                    revenue_circle, lives_lost = match.split(' | ')\n",
    "                    extracted_data.append({\n",
    "                        'district': district,\n",
    "                        'revenue_circle': revenue_circle.strip(),\n",
    "                        lives_lost_col: int(lives_lost)\n",
    "                    })\n",
    "            \n",
    "            return pd.DataFrame(extracted_data)\n",
    "\n",
    "        # Extract lives lost for both 'confirmed' and 'missing'\n",
    "        df_confirmed = extract_details(df['confirmed'], 'lives_lost_confirmed')\n",
    "        df_missing = extract_details(df['missing'], 'lives_lost_missing')\n",
    "\n",
    "        # Merge the two DataFrames on 'district' and 'revenue_circle'\n",
    "        hll_final = pd.merge(df_confirmed, df_missing, on=['district', 'revenue_circle'], how='outer')\n",
    "\n",
    "        # INFRASTRUCTURE\n",
    "        inf = data_file[\"infDamageDetails\"]\n",
    "        \n",
    "        extracted_data = []\n",
    "\n",
    "        # Define the list of indicator types to iterate over\n",
    "        indicator_types = [\"embBreached\", \"embAffected\", \"bridgeAffected\", \"roadAffected\"]\n",
    "\n",
    "        # Iterate through each indicator type in the data\n",
    "        for indicator in indicator_types:\n",
    "            if indicator in inf:\n",
    "                for entry in inf[indicator]:\n",
    "                    district = entry['district']\n",
    "                    for detail in entry['details']:\n",
    "                        # Extract the block name and clean up the indicator value (remove parentheses)\n",
    "                        revenue_circle = detail['block'].split('|')[0].strip('() ')\n",
    "                        indicator_value = detail['block'].split('|')[1].strip('() ')\n",
    "                        \n",
    "                        # Convert the cleaned value to an integer\n",
    "                        indicator_value = int(indicator_value)\n",
    "                        \n",
    "                        # Append a dictionary with the extracted data\n",
    "                        extracted_data.append({\n",
    "                            'district': district,\n",
    "                            'revenue_circle': revenue_circle,\n",
    "                            'indicator': indicator,\n",
    "                            'value': indicator_value\n",
    "                        })\n",
    "\n",
    "        # Convert the list to a DataFrame\n",
    "        inf_dmg = pd.DataFrame(extracted_data)\n",
    "\n",
    "        # Pivot the table to get one row per block, and each indicator as a separate column\n",
    "        inf_dmg = inf_dmg.pivot_table(index=['district', 'revenue_circle'], columns='indicator', values='value', fill_value=0).reset_index()\n",
    "\n",
    "        # Flatten the multi-level column index after pivot\n",
    "        inf_dmg.columns.name = None  # Remove the index name from columns\n",
    "        inf_dmg.columns = [col if isinstance(col, str) else col[1] for col in inf_dmg.columns]\n",
    "        \n",
    "        # Combine all dataframes into one\n",
    "        combined_df = pd.merge(pop_crop, hll_final, on=['revenue_circle', 'district'])\n",
    "        final_df = pd.merge(combined_df, inf_dmg, on=['revenue_circle', 'district'])\n",
    "        final_df['timeperiod'] = timeperiod\n",
    "        \n",
    "        return final_df\n",
    "\n",
    "    except KeyError as e:\n",
    "        print(f\"Warning: Missing key {e} in file {file_path}. Skipping this file.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "result = extract_json(data, timeperiod, file_path)\n",
    "if result is not None:\n",
    "    all_dataframes.append(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 124\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mextract_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeperiod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    126\u001b[0m     all_dataframes\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m, in \u001b[0;36mextract_json\u001b[1;34m(data, timeperiod, file_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_json\u001b[39m(data, timeperiod, file_path):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;66;03m# Normalize specific fields from JSON and create a dataframe\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m         data_file \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m         affected_pop \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data_file[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maffectedPopulation\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     13\u001b[0m         data_list \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\saura\\anaconda3\\envs\\cdl-env\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_hook\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_float\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_int\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_int\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_constant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_constant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_pairs_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saura\\anaconda3\\envs\\cdl-env\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32mc:\\Users\\saura\\anaconda3\\envs\\cdl-env\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\saura\\anaconda3\\envs\\cdl-env\\Lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# Function to flatten the JSON into a dataframe\n",
    "def extract_json(data, timeperiod):\n",
    "    # Normalize specific fields from JSON and create a dataframe\n",
    "    data_file = json.load(data)\n",
    "\n",
    "    affected_pop = pd.DataFrame(data_file[\"affectedPopulation\"])\n",
    "\n",
    "    data_list = []\n",
    "\n",
    "    # Iterate over each entry in the 'details' Series\n",
    "    for index, row in affected_pop.iterrows():\n",
    "        district = row['district']  # Capture the district for the current row\n",
    "        details = row['details']  # Capture the details for the current row\n",
    "        \n",
    "        # Find matches for the current details string\n",
    "        match = re.findall(r'\\(\\s*(.*?)\\s*\\)', details)\n",
    "        \n",
    "        for m in match:\n",
    "            try:\n",
    "                # Split the extracted string by ' | ' to get the revenue circle, population affected, and crop area\n",
    "                revenue_circle, population_affected, crop_area = m.split(' | ')\n",
    "                population_affected = int(re.search(r'\\d+', population_affected).group())\n",
    "                crop_area = float(re.search(r'\\d+(\\.\\d+)?', crop_area).group())\n",
    "                \n",
    "                # Append the results as a dictionary to the list, including the correct district\n",
    "                data_list.append({\n",
    "                    \"district\": district,  # Add the correct district value here\n",
    "                    \"revenue_circle\": revenue_circle.strip(),\n",
    "                    \"Population Affected\": population_affected,\n",
    "                    \"Crop Area\": crop_area\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(\"Error processing match:\", m, e)\n",
    "\n",
    "            # Create a DataFrame from the collected data\n",
    "    pop_crop = pd.DataFrame(data_list)\n",
    "    #pop_extracted\n",
    "\n",
    "    # human lives lost\n",
    "    hll = data_file[\"hllDetails\"]\n",
    "    df = pd.DataFrame(hll)\n",
    "\n",
    "    # Function to extract the details column into a structured DataFrame\n",
    "    def extract_details(df_column, lives_lost_col):\n",
    "        extracted_data = []\n",
    "        \n",
    "        for row in df_column:\n",
    "            district = row['district']\n",
    "            details = row['details']\n",
    "            # Extract the revenue circle and lives lost information\n",
    "            for match in details.split('), '):\n",
    "                match = match.strip('()')\n",
    "                revenue_circle, lives_lost = match.split(' | ')\n",
    "                extracted_data.append({\n",
    "                    'district': district,\n",
    "                    'revenue_circle': revenue_circle.strip(),\n",
    "                    lives_lost_col: int(lives_lost)\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(extracted_data)\n",
    "\n",
    "    # Extract lives lost for both 'confirmed' and 'missing'\n",
    "    df_confirmed = extract_details(df['confirmed'], 'lives_lost_confirmed')\n",
    "    df_missing = extract_details(df['missing'], 'lives_lost_missing')\n",
    "\n",
    "    # Merge the two DataFrames on 'district' and 'revenue_circle'\n",
    "    hll_final = pd.merge(df_confirmed, df_missing, on=['district', 'revenue_circle'], how='outer')\n",
    "\n",
    "    # INFRASTRUCTURE\n",
    "    inf = data_file[\"infDamageDetails\"]\n",
    "    \n",
    "    #inf = data_file[\"infDamageDetails\"]\n",
    "    extracted_data = []\n",
    "\n",
    "    # Define the list of indicator types to iterate over\n",
    "    indicator_types = [\"embBreached\", \"embAffected\", \"bridgeAffected\", \"roadAffected\"]\n",
    "\n",
    "    # Iterate through each indicator type in the data\n",
    "    for indicator in indicator_types:\n",
    "        if indicator in inf:\n",
    "            for entry in inf[indicator]:\n",
    "                district = entry['district']\n",
    "                for detail in entry['details']:\n",
    "                    # Extract the block name and clean up the indicator value (remove parentheses)\n",
    "                    revenue_circle = detail['block'].split('|')[0].strip('() ')\n",
    "                    indicator_value = detail['block'].split('|')[1].strip('() ')\n",
    "                    \n",
    "                    # Convert the cleaned value to an integer\n",
    "                    indicator_value = int(indicator_value)\n",
    "                    \n",
    "                    # Append a dictionary with the extracted data\n",
    "                    extracted_data.append({\n",
    "                        'district': district,\n",
    "                        'revenue_circle': revenue_circle,\n",
    "                        'indicator': indicator,\n",
    "                        'value': indicator_value\n",
    "                    })\n",
    "\n",
    "    # Convert the list to a DataFrame\n",
    "    inf_dmg = pd.DataFrame(extracted_data)\n",
    "\n",
    "    # Pivot the table to get one row per block, and each indicator as a separate column\n",
    "    inf_dmg = inf_dmg.pivot_table(index=['district', 'revenue_circle'], columns='indicator', values='value', fill_value=0).reset_index()\n",
    "\n",
    "    # Flatten the multi-level column index after pivot\n",
    "    inf_dmg.columns.name = None  # Remove the index name from columns\n",
    "    inf_dmg.columns = [col if isinstance(col, str) else col[1] for col in inf_dmg.columns]\n",
    "    \n",
    "    # Add the timeperiod column to each dataframe\n",
    "\n",
    "    # Combine all dataframes into one\n",
    "    combined_df = pd.merge(pop_crop,hll_final,on=['revenue_circle','district'])\n",
    "    final_df = pd.merge(combined_df,inf_dmg,on=['revenue_circle','district'])\n",
    "    final_df['timeperiod']=timeperiod\n",
    "    #combined_df = pd.concat([pop_crop, hll_final, inf_dmg,], ignore_index=True)\n",
    "    \n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2021_05.json\n",
      "Warning: Missing key 'revenue_circle' in file D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2021_05.json. Skipping this file.\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2021_06.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2021_07.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2021_08.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2021_09.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2021_10.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2022_04.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2022_05.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2022_06.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2022_07.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2022_08.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2022_09.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2022_10.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2022_11.json\n",
      "Warning: Missing key 'district' in file D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2022_11.json. Skipping this file.\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2022_12.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2023_02.json\n",
      "Warning: Missing key 'district' in file D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2023_02.json. Skipping this file.\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2023_03.json\n",
      "Warning: Missing key 'district' in file D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2023_03.json. Skipping this file.\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2023_05.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2023_06.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2023_07.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2023_08.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2023_09.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2024_06.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2024_07.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2024_08.json\n",
      "D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\2024_09.json\n"
     ]
    }
   ],
   "source": [
    "all_dataframes = []\n",
    "\n",
    "# Process each JSON file in the folder\n",
    "for file_path in glob.glob(os.path.join(folder_path, '*.json')):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    data = open(file_path)\n",
    "    print(file_path)\n",
    "    timeperiod = extract_timeperiod(file_path)\n",
    "    # Extract timeperiod from the filename\n",
    "    \n",
    "    # Flatten the JSON and process it\n",
    "    df = extract_json(data, timeperiod, file_path)\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    all_dataframes.append(df)\n",
    "\n",
    "combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "# Save the combined dataframe to a CSV file (optional)\n",
    "combined_df.to_csv(r'D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\DRIMS_api_output\\combined_loss_damage_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district</th>\n",
       "      <th>revenue_circle</th>\n",
       "      <th>Population Affected</th>\n",
       "      <th>Crop Area</th>\n",
       "      <th>lives_lost_confirmed</th>\n",
       "      <th>lives_lost_missing</th>\n",
       "      <th>bridgeAffected</th>\n",
       "      <th>embAffected</th>\n",
       "      <th>embBreached</th>\n",
       "      <th>roadAffected</th>\n",
       "      <th>timeperiod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bajali</td>\n",
       "      <td>Bajali</td>\n",
       "      <td>161000</td>\n",
       "      <td>1216.00</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>2022_06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bajali</td>\n",
       "      <td>Sarupeta</td>\n",
       "      <td>195000</td>\n",
       "      <td>857.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2022_06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Baksa</td>\n",
       "      <td>Baska</td>\n",
       "      <td>2209</td>\n",
       "      <td>35.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2022_06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baksa</td>\n",
       "      <td>Jalah</td>\n",
       "      <td>12645</td>\n",
       "      <td>220.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022_06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baksa</td>\n",
       "      <td>Barama</td>\n",
       "      <td>1507</td>\n",
       "      <td>68.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2022_06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>Lakhimpur</td>\n",
       "      <td>Bihpuria</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024_09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Lakhimpur</td>\n",
       "      <td>Narayanpur</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024_09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>Sivasagar</td>\n",
       "      <td>Demow</td>\n",
       "      <td>0</td>\n",
       "      <td>76.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024_09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>Sivasagar</td>\n",
       "      <td>Sivsagar</td>\n",
       "      <td>0</td>\n",
       "      <td>89.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024_09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>Udalguri</td>\n",
       "      <td>Udalguri</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024_09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1002 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       district revenue_circle  Population Affected  Crop Area  \\\n",
       "0        Bajali         Bajali               161000    1216.00   \n",
       "1        Bajali       Sarupeta               195000     857.00   \n",
       "2         Baksa          Baska                 2209      35.07   \n",
       "3         Baksa          Jalah                12645     220.00   \n",
       "4         Baksa         Barama                 1507      68.00   \n",
       "...         ...            ...                  ...        ...   \n",
       "997   Lakhimpur       Bihpuria                    0       0.00   \n",
       "998   Lakhimpur     Narayanpur                    4       0.00   \n",
       "999   Sivasagar          Demow                    0      76.50   \n",
       "1000  Sivasagar       Sivsagar                    0      89.00   \n",
       "1001   Udalguri       Udalguri                    0       0.00   \n",
       "\n",
       "      lives_lost_confirmed  lives_lost_missing  bridgeAffected  embAffected  \\\n",
       "0                        3                   0            31.0          3.0   \n",
       "1                        0                   0             2.0          2.0   \n",
       "2                        0                   0             0.0          0.0   \n",
       "3                        0                   0             0.0          0.0   \n",
       "4                        0                   0             0.0          1.0   \n",
       "...                    ...                 ...             ...          ...   \n",
       "997                      0                   0             0.0          0.0   \n",
       "998                      0                   0             0.0          0.0   \n",
       "999                      0                   0             0.0          0.0   \n",
       "1000                     0                   0             0.0          0.0   \n",
       "1001                     0                   0             0.0          0.0   \n",
       "\n",
       "      embBreached  roadAffected timeperiod  \n",
       "0             3.0          63.0    2022_06  \n",
       "1             2.0          88.0    2022_06  \n",
       "2             0.0          23.0    2022_06  \n",
       "3             0.0           0.0    2022_06  \n",
       "4             1.0           2.0    2022_06  \n",
       "...           ...           ...        ...  \n",
       "997           0.0           0.0    2024_09  \n",
       "998           1.0           0.0    2024_09  \n",
       "999           0.0           0.0    2024_09  \n",
       "1000          0.0           0.0    2024_09  \n",
       "1001          0.0           0.0    2024_09  \n",
       "\n",
       "[1002 rows x 11 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_gdf = gpd.read_file(r'D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Maps\\Geojson\\assam_rc_2024-11.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue_ci</th>\n",
       "      <th>revenue_cr</th>\n",
       "      <th>HQ</th>\n",
       "      <th>are_new</th>\n",
       "      <th>dtname</th>\n",
       "      <th>object_id</th>\n",
       "      <th>dtcode11</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gossaigaon (Pt)</td>\n",
       "      <td>Gossaigaon (Pt)</td>\n",
       "      <td>None</td>\n",
       "      <td>1069</td>\n",
       "      <td>KOKRAJHAR</td>\n",
       "      <td>18-300-00101</td>\n",
       "      <td>18-300</td>\n",
       "      <td>MULTIPOLYGON (((90.14118 26.74010, 90.15342 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bhowraguri</td>\n",
       "      <td>Bhawraguri</td>\n",
       "      <td>None</td>\n",
       "      <td>159</td>\n",
       "      <td>KOKRAJHAR</td>\n",
       "      <td>18-300-00102</td>\n",
       "      <td>18-300</td>\n",
       "      <td>MULTIPOLYGON (((90.09811 26.46455, 90.10012 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dotoma</td>\n",
       "      <td>Dotoma</td>\n",
       "      <td>None</td>\n",
       "      <td>304</td>\n",
       "      <td>KOKRAJHAR</td>\n",
       "      <td>18-300-00103</td>\n",
       "      <td>18-300</td>\n",
       "      <td>MULTIPOLYGON (((90.19819 26.63409, 90.21635 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kokrajhar (Pt)</td>\n",
       "      <td>Kokrajhar (Pt)</td>\n",
       "      <td>y</td>\n",
       "      <td>990</td>\n",
       "      <td>KOKRAJHAR</td>\n",
       "      <td>18-300-00104</td>\n",
       "      <td>18-300</td>\n",
       "      <td>MULTIPOLYGON (((90.33590 26.86280, 90.33610 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bagribari (Pt)</td>\n",
       "      <td>Bagribari (Pt)</td>\n",
       "      <td>None</td>\n",
       "      <td>281</td>\n",
       "      <td>KOKRAJHAR</td>\n",
       "      <td>18-300-00105</td>\n",
       "      <td>18-300</td>\n",
       "      <td>MULTIPOLYGON (((89.99553 26.35026, 89.99717 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Sapekhati</td>\n",
       "      <td>Sapekhati</td>\n",
       "      <td>None</td>\n",
       "      <td>394</td>\n",
       "      <td>CHARAIDEO</td>\n",
       "      <td>18-755-00278</td>\n",
       "      <td>18-755</td>\n",
       "      <td>MULTIPOLYGON (((95.10936 27.12364, 95.10927 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Sonari</td>\n",
       "      <td>Sonari</td>\n",
       "      <td>y</td>\n",
       "      <td>385</td>\n",
       "      <td>CHARAIDEO</td>\n",
       "      <td>18-755-00279</td>\n",
       "      <td>18-755</td>\n",
       "      <td>MULTIPOLYGON (((94.95742 27.06354, 94.95822 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Ujani Majuli</td>\n",
       "      <td>Ujani Majuli</td>\n",
       "      <td>None</td>\n",
       "      <td>322</td>\n",
       "      <td>MAJULI</td>\n",
       "      <td>18-760-00280</td>\n",
       "      <td>18-760</td>\n",
       "      <td>MULTIPOLYGON (((94.33804 26.89186, 94.33850 26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Majuli</td>\n",
       "      <td>Majuli</td>\n",
       "      <td>None</td>\n",
       "      <td>648</td>\n",
       "      <td>MAJULI</td>\n",
       "      <td>18-760-00281</td>\n",
       "      <td>18-760</td>\n",
       "      <td>MULTIPOLYGON (((94.56453 27.18172, 94.56755 27...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Phuloni</td>\n",
       "      <td>Phuloni</td>\n",
       "      <td>None</td>\n",
       "      <td>3303</td>\n",
       "      <td>KARBI ANGLONG</td>\n",
       "      <td>18-314-00275</td>\n",
       "      <td>18-314</td>\n",
       "      <td>MULTIPOLYGON (((92.88407 26.38783, 92.88894 26...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          revenue_ci       revenue_cr    HQ  are_new         dtname  \\\n",
       "0    Gossaigaon (Pt)  Gossaigaon (Pt)  None     1069      KOKRAJHAR   \n",
       "1         Bhowraguri       Bhawraguri  None      159      KOKRAJHAR   \n",
       "2             Dotoma           Dotoma  None      304      KOKRAJHAR   \n",
       "3     Kokrajhar (Pt)   Kokrajhar (Pt)     y      990      KOKRAJHAR   \n",
       "4     Bagribari (Pt)   Bagribari (Pt)  None      281      KOKRAJHAR   \n",
       "..               ...              ...   ...      ...            ...   \n",
       "175        Sapekhati        Sapekhati  None      394      CHARAIDEO   \n",
       "176           Sonari           Sonari     y      385      CHARAIDEO   \n",
       "177     Ujani Majuli     Ujani Majuli  None      322         MAJULI   \n",
       "178           Majuli           Majuli  None      648         MAJULI   \n",
       "179          Phuloni          Phuloni  None     3303  KARBI ANGLONG   \n",
       "\n",
       "        object_id dtcode11                                           geometry  \n",
       "0    18-300-00101   18-300  MULTIPOLYGON (((90.14118 26.74010, 90.15342 26...  \n",
       "1    18-300-00102   18-300  MULTIPOLYGON (((90.09811 26.46455, 90.10012 26...  \n",
       "2    18-300-00103   18-300  MULTIPOLYGON (((90.19819 26.63409, 90.21635 26...  \n",
       "3    18-300-00104   18-300  MULTIPOLYGON (((90.33590 26.86280, 90.33610 26...  \n",
       "4    18-300-00105   18-300  MULTIPOLYGON (((89.99553 26.35026, 89.99717 26...  \n",
       "..            ...      ...                                                ...  \n",
       "175  18-755-00278   18-755  MULTIPOLYGON (((95.10936 27.12364, 95.10927 27...  \n",
       "176  18-755-00279   18-755  MULTIPOLYGON (((94.95742 27.06354, 94.95822 27...  \n",
       "177  18-760-00280   18-760  MULTIPOLYGON (((94.33804 26.89186, 94.33850 26...  \n",
       "178  18-760-00281   18-760  MULTIPOLYGON (((94.56453 27.18172, 94.56755 27...  \n",
       "179  18-314-00275   18-314  MULTIPOLYGON (((92.88407 26.38783, 92.88894 26...  \n",
       "\n",
       "[180 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.rename(columns={\n",
    "    'Population Affected': 'Population_affected_Total',\n",
    "    'Crop Area': 'Crop_Area',\n",
    "    'bridgeAffected': 'Bridge',\n",
    "    'lives_lost_confirmed': 'Human_Live_Lost',\n",
    "    'lives_lost_missing': 'Human_Live_Missing',\n",
    "    'embBreached': 'Embankment breached',\n",
    "    'embAffected': 'Embankments affected',\n",
    "    'roadAffected': 'Roads'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district</th>\n",
       "      <th>revenue_circle</th>\n",
       "      <th>Population_affected_Total</th>\n",
       "      <th>Crop_Area</th>\n",
       "      <th>Human_Live_Lost</th>\n",
       "      <th>Human_Live_Missing</th>\n",
       "      <th>Bridge</th>\n",
       "      <th>Embankments affected</th>\n",
       "      <th>Embankment breached</th>\n",
       "      <th>Roads</th>\n",
       "      <th>timeperiod</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barpeta</td>\n",
       "      <td>Kalgachia</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021_06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barpeta</td>\n",
       "      <td>Barpeta</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021_06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barpeta</td>\n",
       "      <td>Chenga</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021_06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biswanath</td>\n",
       "      <td>Halem</td>\n",
       "      <td>11</td>\n",
       "      <td>35.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021_06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biswanath</td>\n",
       "      <td>Gohpur</td>\n",
       "      <td>0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021_06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>Lakhimpur</td>\n",
       "      <td>Bihpuria</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024_09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>Lakhimpur</td>\n",
       "      <td>Narayanpur</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024_09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>Sivasagar</td>\n",
       "      <td>Demow</td>\n",
       "      <td>0</td>\n",
       "      <td>76.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024_09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>Sivasagar</td>\n",
       "      <td>Sivsagar</td>\n",
       "      <td>0</td>\n",
       "      <td>89.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024_09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>Udalguri</td>\n",
       "      <td>Udalguri</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024_09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1425 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       district revenue_circle  Population_affected_Total  Crop_Area  \\\n",
       "0       Barpeta      Kalgachia                          0       1.73   \n",
       "1       Barpeta        Barpeta                          0       0.00   \n",
       "2       Barpeta         Chenga                          0       0.00   \n",
       "3     Biswanath          Halem                         11      35.50   \n",
       "4     Biswanath         Gohpur                          0       8.50   \n",
       "...         ...            ...                        ...        ...   \n",
       "1420  Lakhimpur       Bihpuria                          0       0.00   \n",
       "1421  Lakhimpur     Narayanpur                          4       0.00   \n",
       "1422  Sivasagar          Demow                          0      76.50   \n",
       "1423  Sivasagar       Sivsagar                          0      89.00   \n",
       "1424   Udalguri       Udalguri                          0       0.00   \n",
       "\n",
       "      Human_Live_Lost  Human_Live_Missing  Bridge  Embankments affected  \\\n",
       "0                   0                   0     0.0                   0.0   \n",
       "1                   1                   0     0.0                   0.0   \n",
       "2                   1                   0     0.0                   0.0   \n",
       "3                   0                   0     0.0                   2.0   \n",
       "4                   0                   0     0.0                   0.0   \n",
       "...               ...                 ...     ...                   ...   \n",
       "1420                0                   0     0.0                   0.0   \n",
       "1421                0                   0     0.0                   0.0   \n",
       "1422                0                   0     0.0                   0.0   \n",
       "1423                0                   0     0.0                   0.0   \n",
       "1424                0                   0     0.0                   0.0   \n",
       "\n",
       "      Embankment breached  Roads timeperiod  \n",
       "0                     0.0    0.0    2021_06  \n",
       "1                     0.0    0.0    2021_06  \n",
       "2                     0.0    0.0    2021_06  \n",
       "3                     2.0    2.0    2021_06  \n",
       "4                     1.0    0.0    2021_06  \n",
       "...                   ...    ...        ...  \n",
       "1420                  0.0    0.0    2024_09  \n",
       "1421                  1.0    0.0    2024_09  \n",
       "1422                  0.0    0.0    2024_09  \n",
       "1423                  0.0    0.0    2024_09  \n",
       "1424                  0.0    0.0    2024_09  \n",
       "\n",
       "[1425 rows x 11 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_merge(df_1, df_2, key1, key2, threshold=90, limit=2):\n",
    "    \"\"\"\n",
    "    :param df_1: the left table to join\n",
    "    :param df_2: the right table to join\n",
    "    :param key1: key column of the left table\n",
    "    :param key2: key column of the right table\n",
    "    :param threshold: how close the matches should be to return a match, based on Levenshtein distance\n",
    "    :param limit: the amount of matches that will get returned, these are sorted high to low\n",
    "    :return: dataframe with boths keys and matches\n",
    "    \"\"\"\n",
    "    s = df_2[key2].tolist()\n",
    "\n",
    "    m = df_1[key1].apply(lambda x: process.extract(x, s, limit=limit))    \n",
    "    df_1['matches'] = m\n",
    "\n",
    "    m2 = df_1['matches'].apply(lambda x: ', '.join([i[0] for i in x if i[1] >= threshold]))\n",
    "    df_1['matches'] = m2\n",
    "\n",
    "    return df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue_ci</th>\n",
       "      <th>revenue_cr</th>\n",
       "      <th>HQ</th>\n",
       "      <th>are_new</th>\n",
       "      <th>dtname</th>\n",
       "      <th>object_id</th>\n",
       "      <th>dtcode11</th>\n",
       "      <th>geometry</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gossaigaon (Pt)</td>\n",
       "      <td>Gossaigaon (Pt)</td>\n",
       "      <td>None</td>\n",
       "      <td>1069</td>\n",
       "      <td>KOKRAJHAR</td>\n",
       "      <td>18-300-00101</td>\n",
       "      <td>18-300</td>\n",
       "      <td>MULTIPOLYGON (((90.14118 26.74010, 90.15342 26...</td>\n",
       "      <td>Gossaigaon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bhowraguri</td>\n",
       "      <td>Bhawraguri</td>\n",
       "      <td>None</td>\n",
       "      <td>159</td>\n",
       "      <td>KOKRAJHAR</td>\n",
       "      <td>18-300-00102</td>\n",
       "      <td>18-300</td>\n",
       "      <td>MULTIPOLYGON (((90.09811 26.46455, 90.10012 26...</td>\n",
       "      <td>Bhawraguri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dotoma</td>\n",
       "      <td>Dotoma</td>\n",
       "      <td>None</td>\n",
       "      <td>304</td>\n",
       "      <td>KOKRAJHAR</td>\n",
       "      <td>18-300-00103</td>\n",
       "      <td>18-300</td>\n",
       "      <td>MULTIPOLYGON (((90.19819 26.63409, 90.21635 26...</td>\n",
       "      <td>Dotma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kokrajhar (Pt)</td>\n",
       "      <td>Kokrajhar (Pt)</td>\n",
       "      <td>y</td>\n",
       "      <td>990</td>\n",
       "      <td>KOKRAJHAR</td>\n",
       "      <td>18-300-00104</td>\n",
       "      <td>18-300</td>\n",
       "      <td>MULTIPOLYGON (((90.33590 26.86280, 90.33610 26...</td>\n",
       "      <td>Kokrajhar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bagribari (Pt)</td>\n",
       "      <td>Bagribari (Pt)</td>\n",
       "      <td>None</td>\n",
       "      <td>281</td>\n",
       "      <td>KOKRAJHAR</td>\n",
       "      <td>18-300-00105</td>\n",
       "      <td>18-300</td>\n",
       "      <td>MULTIPOLYGON (((89.99553 26.35026, 89.99717 26...</td>\n",
       "      <td>Bagribari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Sapekhati</td>\n",
       "      <td>Sapekhati</td>\n",
       "      <td>None</td>\n",
       "      <td>394</td>\n",
       "      <td>CHARAIDEO</td>\n",
       "      <td>18-755-00278</td>\n",
       "      <td>18-755</td>\n",
       "      <td>MULTIPOLYGON (((95.10936 27.12364, 95.10927 27...</td>\n",
       "      <td>Sapekhati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Sonari</td>\n",
       "      <td>Sonari</td>\n",
       "      <td>y</td>\n",
       "      <td>385</td>\n",
       "      <td>CHARAIDEO</td>\n",
       "      <td>18-755-00279</td>\n",
       "      <td>18-755</td>\n",
       "      <td>MULTIPOLYGON (((94.95742 27.06354, 94.95822 27...</td>\n",
       "      <td>Sonari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Ujani Majuli</td>\n",
       "      <td>Ujani Majuli</td>\n",
       "      <td>None</td>\n",
       "      <td>322</td>\n",
       "      <td>MAJULI</td>\n",
       "      <td>18-760-00280</td>\n",
       "      <td>18-760</td>\n",
       "      <td>MULTIPOLYGON (((94.33804 26.89186, 94.33850 26...</td>\n",
       "      <td>Ujani Majuli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Majuli</td>\n",
       "      <td>Majuli</td>\n",
       "      <td>None</td>\n",
       "      <td>648</td>\n",
       "      <td>MAJULI</td>\n",
       "      <td>18-760-00281</td>\n",
       "      <td>18-760</td>\n",
       "      <td>MULTIPOLYGON (((94.56453 27.18172, 94.56755 27...</td>\n",
       "      <td>Majuli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Phuloni</td>\n",
       "      <td>Phuloni</td>\n",
       "      <td>None</td>\n",
       "      <td>3303</td>\n",
       "      <td>KARBI ANGLONG</td>\n",
       "      <td>18-314-00275</td>\n",
       "      <td>18-314</td>\n",
       "      <td>MULTIPOLYGON (((92.88407 26.38783, 92.88894 26...</td>\n",
       "      <td>Phuloni</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          revenue_ci       revenue_cr    HQ  are_new         dtname  \\\n",
       "0    Gossaigaon (Pt)  Gossaigaon (Pt)  None     1069      KOKRAJHAR   \n",
       "1         Bhowraguri       Bhawraguri  None      159      KOKRAJHAR   \n",
       "2             Dotoma           Dotoma  None      304      KOKRAJHAR   \n",
       "3     Kokrajhar (Pt)   Kokrajhar (Pt)     y      990      KOKRAJHAR   \n",
       "4     Bagribari (Pt)   Bagribari (Pt)  None      281      KOKRAJHAR   \n",
       "..               ...              ...   ...      ...            ...   \n",
       "175        Sapekhati        Sapekhati  None      394      CHARAIDEO   \n",
       "176           Sonari           Sonari     y      385      CHARAIDEO   \n",
       "177     Ujani Majuli     Ujani Majuli  None      322         MAJULI   \n",
       "178           Majuli           Majuli  None      648         MAJULI   \n",
       "179          Phuloni          Phuloni  None     3303  KARBI ANGLONG   \n",
       "\n",
       "        object_id dtcode11                                           geometry  \\\n",
       "0    18-300-00101   18-300  MULTIPOLYGON (((90.14118 26.74010, 90.15342 26...   \n",
       "1    18-300-00102   18-300  MULTIPOLYGON (((90.09811 26.46455, 90.10012 26...   \n",
       "2    18-300-00103   18-300  MULTIPOLYGON (((90.19819 26.63409, 90.21635 26...   \n",
       "3    18-300-00104   18-300  MULTIPOLYGON (((90.33590 26.86280, 90.33610 26...   \n",
       "4    18-300-00105   18-300  MULTIPOLYGON (((89.99553 26.35026, 89.99717 26...   \n",
       "..            ...      ...                                                ...   \n",
       "175  18-755-00278   18-755  MULTIPOLYGON (((95.10936 27.12364, 95.10927 27...   \n",
       "176  18-755-00279   18-755  MULTIPOLYGON (((94.95742 27.06354, 94.95822 27...   \n",
       "177  18-760-00280   18-760  MULTIPOLYGON (((94.33804 26.89186, 94.33850 26...   \n",
       "178  18-760-00281   18-760  MULTIPOLYGON (((94.56453 27.18172, 94.56755 27...   \n",
       "179  18-314-00275   18-314  MULTIPOLYGON (((92.88407 26.38783, 92.88894 26...   \n",
       "\n",
       "          matches  \n",
       "0      Gossaigaon  \n",
       "1      Bhawraguri  \n",
       "2           Dotma  \n",
       "3       Kokrajhar  \n",
       "4       Bagribari  \n",
       "..            ...  \n",
       "175     Sapekhati  \n",
       "176        Sonari  \n",
       "177  Ujani Majuli  \n",
       "178        Majuli  \n",
       "179       Phuloni  \n",
       "\n",
       "[180 rows x 9 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzymatch = fuzzy_merge(rc_gdf, combined_df, 'revenue_ci', 'revenue_circle', threshold=80,limit=1)\n",
    "fuzzymatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>district</th>\n",
       "      <th>revenue_circle</th>\n",
       "      <th>Population_affected_Total</th>\n",
       "      <th>Crop_Area</th>\n",
       "      <th>Human_Live_Lost</th>\n",
       "      <th>Human_Live_Missing</th>\n",
       "      <th>Bridge</th>\n",
       "      <th>Embankments affected</th>\n",
       "      <th>Embankment breached</th>\n",
       "      <th>Roads</th>\n",
       "      <th>timeperiod</th>\n",
       "      <th>district_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Barpeta</td>\n",
       "      <td>Kalgachia</td>\n",
       "      <td>0</td>\n",
       "      <td>1.73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021_06</td>\n",
       "      <td>BARPETA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Barpeta</td>\n",
       "      <td>Barpeta</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021_06</td>\n",
       "      <td>BARPETA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barpeta</td>\n",
       "      <td>Chenga</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021_06</td>\n",
       "      <td>BARPETA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Biswanath</td>\n",
       "      <td>Halem</td>\n",
       "      <td>11</td>\n",
       "      <td>35.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021_06</td>\n",
       "      <td>BISWANATH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Biswanath</td>\n",
       "      <td>Gohpur</td>\n",
       "      <td>0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021_06</td>\n",
       "      <td>BISWANATH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>Lakhimpur</td>\n",
       "      <td>Bihpuria</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024_09</td>\n",
       "      <td>LAKHIMPUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>Lakhimpur</td>\n",
       "      <td>Narayanpur</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024_09</td>\n",
       "      <td>LAKHIMPUR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>Sivasagar</td>\n",
       "      <td>Demow</td>\n",
       "      <td>0</td>\n",
       "      <td>76.50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024_09</td>\n",
       "      <td>SIVASAGAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>Sivasagar</td>\n",
       "      <td>Sivsagar</td>\n",
       "      <td>0</td>\n",
       "      <td>89.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024_09</td>\n",
       "      <td>SIVASAGAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>Udalguri</td>\n",
       "      <td>Udalguri</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2024_09</td>\n",
       "      <td>UDALGURI</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1425 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       district revenue_circle  Population_affected_Total  Crop_Area  \\\n",
       "0       Barpeta      Kalgachia                          0       1.73   \n",
       "1       Barpeta        Barpeta                          0       0.00   \n",
       "2       Barpeta         Chenga                          0       0.00   \n",
       "3     Biswanath          Halem                         11      35.50   \n",
       "4     Biswanath         Gohpur                          0       8.50   \n",
       "...         ...            ...                        ...        ...   \n",
       "1420  Lakhimpur       Bihpuria                          0       0.00   \n",
       "1421  Lakhimpur     Narayanpur                          4       0.00   \n",
       "1422  Sivasagar          Demow                          0      76.50   \n",
       "1423  Sivasagar       Sivsagar                          0      89.00   \n",
       "1424   Udalguri       Udalguri                          0       0.00   \n",
       "\n",
       "      Human_Live_Lost  Human_Live_Missing  Bridge  Embankments affected  \\\n",
       "0                   0                   0     0.0                   0.0   \n",
       "1                   1                   0     0.0                   0.0   \n",
       "2                   1                   0     0.0                   0.0   \n",
       "3                   0                   0     0.0                   2.0   \n",
       "4                   0                   0     0.0                   0.0   \n",
       "...               ...                 ...     ...                   ...   \n",
       "1420                0                   0     0.0                   0.0   \n",
       "1421                0                   0     0.0                   0.0   \n",
       "1422                0                   0     0.0                   0.0   \n",
       "1423                0                   0     0.0                   0.0   \n",
       "1424                0                   0     0.0                   0.0   \n",
       "\n",
       "      Embankment breached  Roads timeperiod district_2  \n",
       "0                     0.0    0.0    2021_06    BARPETA  \n",
       "1                     0.0    0.0    2021_06    BARPETA  \n",
       "2                     0.0    0.0    2021_06    BARPETA  \n",
       "3                     2.0    2.0    2021_06  BISWANATH  \n",
       "4                     1.0    0.0    2021_06  BISWANATH  \n",
       "...                   ...    ...        ...        ...  \n",
       "1420                  0.0    0.0    2024_09  LAKHIMPUR  \n",
       "1421                  1.0    0.0    2024_09  LAKHIMPUR  \n",
       "1422                  0.0    0.0    2024_09  SIVASAGAR  \n",
       "1423                  0.0    0.0    2024_09  SIVASAGAR  \n",
       "1424                  0.0    0.0    2024_09   UDALGURI  \n",
       "\n",
       "[1425 rows x 12 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['district_2'] = combined_df['district'].str.upper()\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>revenue_ci</th>\n",
       "      <th>object_id</th>\n",
       "      <th>dtname</th>\n",
       "      <th>timeperiod</th>\n",
       "      <th>Bridge</th>\n",
       "      <th>Embankment breached</th>\n",
       "      <th>Embankments affected</th>\n",
       "      <th>Roads</th>\n",
       "      <th>Human_Live_Lost</th>\n",
       "      <th>Population_affected_Total</th>\n",
       "      <th>Crop_Area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Agamoni</td>\n",
       "      <td>18-301-00110</td>\n",
       "      <td>DHUBRI</td>\n",
       "      <td>2022_06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1190.0</td>\n",
       "      <td>893.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Agamoni</td>\n",
       "      <td>18-301-00110</td>\n",
       "      <td>DHUBRI</td>\n",
       "      <td>2022_07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Agamoni</td>\n",
       "      <td>18-301-00110</td>\n",
       "      <td>DHUBRI</td>\n",
       "      <td>2022_08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Agamoni</td>\n",
       "      <td>18-301-00110</td>\n",
       "      <td>DHUBRI</td>\n",
       "      <td>2023_06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>128.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Agamoni</td>\n",
       "      <td>18-301-00110</td>\n",
       "      <td>DHUBRI</td>\n",
       "      <td>2023_07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3336.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>Ujani Majuli</td>\n",
       "      <td>18-760-00280</td>\n",
       "      <td>MAJULI</td>\n",
       "      <td>2023_07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>137.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>Ujani Majuli</td>\n",
       "      <td>18-760-00280</td>\n",
       "      <td>MAJULI</td>\n",
       "      <td>2023_08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12852.0</td>\n",
       "      <td>443.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>Ujani Majuli</td>\n",
       "      <td>18-760-00280</td>\n",
       "      <td>MAJULI</td>\n",
       "      <td>2023_09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>269.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1466</th>\n",
       "      <td>Ujani Majuli</td>\n",
       "      <td>18-760-00280</td>\n",
       "      <td>MAJULI</td>\n",
       "      <td>2024_06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12363.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>Ujani Majuli</td>\n",
       "      <td>18-760-00280</td>\n",
       "      <td>MAJULI</td>\n",
       "      <td>2024_07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14137.0</td>\n",
       "      <td>2123.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1233 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        revenue_ci     object_id  dtname timeperiod  Bridge  \\\n",
       "13         Agamoni  18-301-00110  DHUBRI    2022_06     0.0   \n",
       "14         Agamoni  18-301-00110  DHUBRI    2022_07     0.0   \n",
       "15         Agamoni  18-301-00110  DHUBRI    2022_08     0.0   \n",
       "16         Agamoni  18-301-00110  DHUBRI    2023_06     0.0   \n",
       "17         Agamoni  18-301-00110  DHUBRI    2023_07     0.0   \n",
       "...            ...           ...     ...        ...     ...   \n",
       "1463  Ujani Majuli  18-760-00280  MAJULI    2023_07     0.0   \n",
       "1464  Ujani Majuli  18-760-00280  MAJULI    2023_08     0.0   \n",
       "1465  Ujani Majuli  18-760-00280  MAJULI    2023_09     0.0   \n",
       "1466  Ujani Majuli  18-760-00280  MAJULI    2024_06     0.0   \n",
       "1467  Ujani Majuli  18-760-00280  MAJULI    2024_07     0.0   \n",
       "\n",
       "      Embankment breached  Embankments affected  Roads  Human_Live_Lost  \\\n",
       "13                    0.0                   0.0    9.0              0.0   \n",
       "14                    0.0                   0.0    2.0              0.0   \n",
       "15                    0.0                   0.0    0.0              0.0   \n",
       "16                    0.0                   0.0    0.0              0.0   \n",
       "17                    0.0                   0.0    3.0              0.0   \n",
       "...                   ...                   ...    ...              ...   \n",
       "1463                  0.0                   3.0    3.0              0.0   \n",
       "1464                  0.0                   0.0    3.0              0.0   \n",
       "1465                  0.0                   0.0    0.0              0.0   \n",
       "1466                  0.0                   0.0    0.0              0.0   \n",
       "1467                  0.0                   3.0    3.0              0.0   \n",
       "\n",
       "      Population_affected_Total  Crop_Area  \n",
       "13                       1190.0      893.5  \n",
       "14                          0.0        0.0  \n",
       "15                          0.0       35.5  \n",
       "16                        576.0      128.5  \n",
       "17                       3336.0       60.0  \n",
       "...                         ...        ...  \n",
       "1463                        0.0      137.0  \n",
       "1464                    12852.0      443.0  \n",
       "1465                        0.0      269.0  \n",
       "1466                    12363.0        0.0  \n",
       "1467                    14137.0     2123.0  \n",
       "\n",
       "[1233 rows x 11 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc_complete_matched = fuzzymatch.merge(combined_df, left_on=['matches','dtname'], right_on=['revenue_circle','district_2'], how='outer')#.to_csv('frims_rc_id_mapping.csv', index=False)\n",
    "rc_complete_matched = rc_complete_matched[['revenue_ci', 'object_id', 'dtname', #'revenue_cr',\n",
    "       #'HQ', 'area', 'are_new', 'geometry', 'matches', 'district','revenue_circle', \n",
    "       'Population_affected_Total', 'Crop_Area',\n",
    "       'timeperiod', 'Bridge', 'Embankment breached', 'Embankments affected',\n",
    "       'Roads', 'Human_Live_Lost'\n",
    "       ]]\n",
    "df_cleaned=rc_complete_matched.dropna(subset=['timeperiod','object_id'])\n",
    "df_cleaned = df_cleaned[['revenue_ci','object_id','dtname','timeperiod','Bridge','Embankment breached','Embankments affected','Roads','Human_Live_Lost','Population_affected_Total', 'Crop_Area']]\n",
    "df_cleaned = df_cleaned.rename(columns={'district_2':'DISTRICT'})\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_columns = ['Bridge', 'Embankment breached', 'Embankments affected', 'Roads', \n",
    "                     'Human_Live_Lost', \n",
    "                     'Population_affected_Total', 'Crop_Area']\n",
    "                     #'Total_House_Fully_Damaged', 'Total_Animal_Affected']\n",
    "\n",
    "variable_path = r'D:\\CivicDataLab_IDS-DRR\\IDS-DRR_Github\\Deployment\\flood-data-ecosystem-Assam\\Sources\\DRIMS\\data\\variables/'\n",
    "\n",
    "# Step 2: Loop through each indicator column\n",
    "for indicator in indicator_columns:\n",
    "    # Step 3: Get unique time periods\n",
    "    for timeperiod in df_cleaned['timeperiod'].unique():\n",
    "        # Step 4: Filter the DataFrame by time period and the current indicator column\n",
    "        filtered_df = df_cleaned[['object_id', indicator]][df_cleaned['timeperiod'] == timeperiod]\n",
    "        # Step 5: Create directory if it doesn't exist\n",
    "        if not os.path.exists(indicator):\n",
    "            os.makedirs(indicator)\n",
    "        \n",
    "        # Step 6: Construct file name and save the filtered DataFrame to CSV\n",
    "        filename = f\"{indicator}_{timeperiod}.csv\"\n",
    "        file_path = os.path.join(variable_path,indicator, filename)\n",
    "        filtered_df.to_csv(file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
